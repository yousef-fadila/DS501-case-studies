{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 3 : Textual analysis of movie reviews\n",
    "\n",
    "** Due Date: November 17, 2016 5:59PM**\n",
    "\n",
    "*------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.conversational-technologies.com/nldemos/nlWordle.GIF\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEAM Members:** Please EDIT this cell and add the names of all the team members in your team\n",
    "\n",
    "    member 1\n",
    "    \n",
    "    member 2\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desired outcome of the case study.**\n",
    "* In this case study we will look at movie reviews from the v2.0 polarity dataset comes from\n",
    "the http://www.cs.cornell.edu/people/pabo/movie-review-data.\n",
    "    * It contains written reviews of movies divided into positive and negative reviews.\n",
    "* As in Case Study 2 idea is to *analyze* the data set, make *conjectures*, support or refute those conjectures with *data*, and *tell a story* about the data!\n",
    "    \n",
    "**Required Readings:** \n",
    "* This case study will be based upon the scikit-learn Python library\n",
    "* We will build upon the turtorial \"Working With Text Data\" which can be found at http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "**Case study assumptions:**\n",
    "* You have access to a python installation\n",
    "\n",
    "**Required Python libraries:**\n",
    "* Numpy (www.numpy.org) (should already be installed from Case Study 2)\n",
    "* Matplotlib (matplotlib.org) (should already be installed from Case Study 2)\n",
    "* Scikit-learn (scikit-learn.org) (avaiable from Enthought Canopy)\n",
    "* You are also welcome to use the Python Natural Language Processing Toolkit (www.nltk.org) (though it is not required).\n",
    "\n",
    "** NOTE **\n",
    "* Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost.\n",
    "\n",
    "*----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (20 points): Complete Exercise 2: Sentiment Analysis on movie reviews from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Assuming that you have downloaded the scikit-learn source code:\n",
    "    * The data cane be downloaded using doc/tutorial/text_analytics/data/movie_reviews/fetch_data.py\n",
    "    * A skeleton for the solution can be found in doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py\n",
    "    * A completed solution can be found in doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py\n",
    "* **It is ok to use the solution provided in the scikit-learn distribution as a starting place for your work.**\n",
    "\n",
    "### Modify the solution to Exercise 2 so that it can run in this iPython notebook\n",
    "* This will likely involved moving around data files and/or small modifications to the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from IPython.display import HTML\n",
    "from datetime import datetime\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "lastRunTime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** download the data (if not avaliable locally ) and load it into dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2000\n",
      "Done: 2016-11-15 14:29:15\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Script to download the movie review dataset\"\"\"\n",
    "# copied from doc/tutorial/text_analytics/data/movie_reviews/fetch_data.py\n",
    "import os\n",
    "import tarfile\n",
    "from contextlib import closing\n",
    "try:\n",
    "    from urllib import urlopen\n",
    "except ImportError:\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "\n",
    "URL = (\"http://www.cs.cornell.edu/people/pabo/\"\n",
    "       \"movie-review-data/review_polarity.tar.gz\")\n",
    "\n",
    "ARCHIVE_NAME = URL.rsplit('/', 1)[1]\n",
    "DATA_FOLDER = \"txt_sentoken\"\n",
    "\n",
    "\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "\n",
    "    if not os.path.exists(ARCHIVE_NAME):\n",
    "        print(\"Downloading dataset from %s (3 MB)\" % URL)\n",
    "        opener = urlopen(URL)\n",
    "        with open(ARCHIVE_NAME, 'wb') as archive:\n",
    "            archive.write(opener.read())\n",
    "\n",
    "    print(\"Decompressing %s\" % ARCHIVE_NAME)\n",
    "    with closing(tarfile.open(ARCHIVE_NAME, \"r:gz\")) as archive:\n",
    "        archive.extractall(path='.')\n",
    "    os.remove(ARCHIVE_NAME)\n",
    "    \n",
    "dataset = load_files(DATA_FOLDER, shuffle=False)\n",
    "print(\"n_samples: %d\" % len(dataset.data))\n",
    "\n",
    "print(\"Done: \" + lastRunTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Build the text classification pipeline **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 params - {'vect__ngram_range': (1, 1)}; mean - 0.84; std - 0.01\n",
      "1 params - {'vect__ngram_range': (1, 2)}; mean - 0.87; std - 0.01\n",
      "Done: 2016-11-13 20:05:45\n"
     ]
    }
   ],
   "source": [
    "# split the dataset in training and test set:\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
    "\n",
    "# TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
    "# that are too rare or too frequent\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n",
    "    ('clf', LinearSVC(C=1000)),\n",
    "])\n",
    "\n",
    "# TASK: Build a grid search to find out whether unigrams or bigrams are\n",
    "# more useful.\n",
    "# Fit the pipeline on the training set using grid search for the parameters\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "}\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=1)\n",
    "grid_search.fit(docs_train, y_train)\n",
    "\n",
    "# TASK: print the mean and std for each candidate along with the parameter\n",
    "# settings for all the candidates explored by grid search.\n",
    "n_candidates = len(grid_search.cv_results_['params'])\n",
    "for i in range(n_candidates):\n",
    "    print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
    "             % (grid_search.cv_results_['params'][i],\n",
    "                grid_search.cv_results_['mean_test_score'][i],\n",
    "                grid_search.cv_results_['std_test_score'][i]))\n",
    "    \n",
    "# TASK: Predict the outcome on the testing set and store it in a variable\n",
    "# named y_predicted\n",
    "y_predicted = grid_search.predict(docs_test)\n",
    "\n",
    "print(\"Done: \" + lastRunTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Print the classification report and the confusion matrix **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Classification Report**\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.84      0.83      0.84       249\n",
      "        pos       0.83      0.85      0.84       251\n",
      "\n",
      "avg / total       0.84      0.84      0.84       500\n",
      "\n",
      "**Confusion Matrix**\n",
      "\n",
      "[[206  43]\n",
      " [ 38 213]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAFdCAYAAABGoXXzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQInAApQILECJwAKUCCxAyf8AstncagRDJWgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25028660fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 2016-11-13 20:05:45\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report\n",
    "print(\"**Classification Report**\\n\")\n",
    "print(metrics.classification_report(y_test, y_predicted,\n",
    "                                    target_names=dataset.target_names))\n",
    "\n",
    "# Print and plot the confusion matrix\n",
    "print(\"**Confusion Matrix**\\n\")\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "print(cm)\n",
    "\n",
    "plt.matshow(cm)\n",
    "plt.show()\n",
    "\n",
    "print(\"Done: \" + lastRunTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (20 points): Explore the scikit-learn TfidVectorizer class\n",
    "\n",
    "**Read the documentation for the TfidVectorizer class at http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html.** \n",
    "* Define the term frequencyâ€“inverse document frequency (TF-IDF) statistic (http://en.wikipedia.org/wiki/Tf%E2%80%93idf will likely help).\n",
    "* Run the TfidVectorizer class on the training data above (docs_train).\n",
    "* Explore the min_df and max_df parameters of TfidVectorizer.  What do they mean? How do they change the features you get?\n",
    "* Explore the ngram_range parameter of TfidVectorizer.  What does it mean? How does it change the features you get? (Note, large values  of ngram_range may take a long time to run!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*------------------------\n",
    "\n",
    "## Problem 3 (20 points): Machine learning algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Based upon Problem 2 pick some parameters for TfidfVectorizer\n",
    "    * \"fit\" your TfidfVectorizer using docs_train\n",
    "    * Compute \"Xtrain\", a Tf-idf-weighted document-term matrix using the transform function on docs_train\n",
    "    * Compute \"Xtest\", a Tf-idf-weighted document-term matrix using the transform function on docs_test\n",
    "    * Note, be sure to use the same Tf-idf-weighted class (**\"fit\" using docs_train**) to transform **both** docs_test and docs_train\n",
    "* Examine two classifiers provided by scikit-learn \n",
    "    * LinearSVC\n",
    "    * KNeighborsClassifier\n",
    "    * Try a number of different parameter settings for each and judge your performance using a confusion matrix (see Problem 1 for an example).\n",
    "* Does one classifier, or one set of parameters work better?\n",
    "    * Why do you think it might be working better?\n",
    "* For a particular choice of parameters and classifier, look at 2 examples where the prediction was incorrect.\n",
    "    * Can you conjecture on why the classifier made a mistake for this prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** \"fit\" your TfidfVectorizer using docs_train\n",
    "Compute \"Xtrain\", a Tf-idf-weighted document-term matrix using the transform function on docs_train\n",
    "Compute \"Xtest\", a Tf-idf-weighted document-term matrix using the transform function on docs_test **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain:(1500, 436255)\n",
      "Xtest:(500, 436255)\n"
     ]
    }
   ],
   "source": [
    "min_df = 1\n",
    "max_df = 0.95\n",
    "ngram_range = (1, 2)\n",
    "\n",
    "q3_docs_train, q3_docs_test, q3_y_train, q3_y_test = train_test_split(\n",
    "    dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
    "\n",
    "TfidfVect= TfidfVectorizer(min_df = min_df, max_df = max_df, ngram_range = ngram_range).fit(q3_docs_train)\n",
    "Xtrain = TfidfVect.transform(q3_docs_train).toarray()\n",
    "print(\"Xtrain:\" + str(Xtrain.shape))\n",
    "Xtest = TfidfVect.transform(q3_docs_test).toarray()\n",
    "print(\"Xtest:\" + str(Xtest.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** LinearSVC Examination **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 params - {'clf__tol': 1e-05, 'clf__C': 0.01}; mean - 0.61; std - 0.01\n",
      "1 params - {'clf__tol': 0.0001, 'clf__C': 0.01}; mean - 0.61; std - 0.01\n",
      "2 params - {'clf__tol': 0.001, 'clf__C': 0.01}; mean - 0.61; std - 0.01\n",
      "3 params - {'clf__tol': 0.01, 'clf__C': 0.01}; mean - 0.61; std - 0.01\n",
      "4 params - {'clf__tol': 1, 'clf__C': 0.01}; mean - 0.51; std - 0.00\n",
      "5 params - {'clf__tol': 10, 'clf__C': 0.01}; mean - 0.59; std - 0.13\n",
      "6 params - {'clf__tol': 1e-05, 'clf__C': 0.1}; mean - 0.81; std - 0.01\n",
      "7 params - {'clf__tol': 0.0001, 'clf__C': 0.1}; mean - 0.81; std - 0.01\n",
      "8 params - {'clf__tol': 0.001, 'clf__C': 0.1}; mean - 0.81; std - 0.01\n",
      "9 params - {'clf__tol': 0.01, 'clf__C': 0.1}; mean - 0.81; std - 0.01\n",
      "10 params - {'clf__tol': 1, 'clf__C': 0.1}; mean - 0.81; std - 0.01\n",
      "11 params - {'clf__tol': 10, 'clf__C': 0.1}; mean - 0.55; std - 0.06\n",
      "12 params - {'clf__tol': 1e-05, 'clf__C': 0.5}; mean - 0.83; std - 0.01\n",
      "13 params - {'clf__tol': 0.0001, 'clf__C': 0.5}; mean - 0.83; std - 0.01\n",
      "14 params - {'clf__tol': 0.001, 'clf__C': 0.5}; mean - 0.83; std - 0.01\n",
      "15 params - {'clf__tol': 0.01, 'clf__C': 0.5}; mean - 0.83; std - 0.01\n",
      "16 params - {'clf__tol': 1, 'clf__C': 0.5}; mean - 0.83; std - 0.02\n",
      "17 params - {'clf__tol': 10, 'clf__C': 0.5}; mean - 0.58; std - 0.05\n",
      "18 params - {'clf__tol': 1e-05, 'clf__C': 1}; mean - 0.83; std - 0.01\n",
      "19 params - {'clf__tol': 0.0001, 'clf__C': 1}; mean - 0.83; std - 0.01\n",
      "20 params - {'clf__tol': 0.001, 'clf__C': 1}; mean - 0.83; std - 0.01\n",
      "21 params - {'clf__tol': 0.01, 'clf__C': 1}; mean - 0.83; std - 0.01\n",
      "22 params - {'clf__tol': 1, 'clf__C': 1}; mean - 0.82; std - 0.01\n",
      "23 params - {'clf__tol': 10, 'clf__C': 1}; mean - 0.74; std - 0.08\n",
      "24 params - {'clf__tol': 1e-05, 'clf__C': 10}; mean - 0.83; std - 0.01\n",
      "25 params - {'clf__tol': 0.0001, 'clf__C': 10}; mean - 0.83; std - 0.01\n",
      "26 params - {'clf__tol': 0.001, 'clf__C': 10}; mean - 0.83; std - 0.01\n",
      "27 params - {'clf__tol': 0.01, 'clf__C': 10}; mean - 0.83; std - 0.01\n",
      "28 params - {'clf__tol': 1, 'clf__C': 10}; mean - 0.83; std - 0.01\n",
      "29 params - {'clf__tol': 10, 'clf__C': 10}; mean - 0.54; std - 0.05\n",
      "30 params - {'clf__tol': 1e-05, 'clf__C': 100}; mean - 0.84; std - 0.01\n",
      "31 params - {'clf__tol': 0.0001, 'clf__C': 100}; mean - 0.84; std - 0.01\n",
      "32 params - {'clf__tol': 0.001, 'clf__C': 100}; mean - 0.84; std - 0.01\n",
      "33 params - {'clf__tol': 0.01, 'clf__C': 100}; mean - 0.84; std - 0.01\n",
      "34 params - {'clf__tol': 1, 'clf__C': 100}; mean - 0.83; std - 0.01\n",
      "35 params - {'clf__tol': 10, 'clf__C': 100}; mean - 0.62; std - 0.13\n",
      "36 params - {'clf__tol': 1e-05, 'clf__C': 1000}; mean - 0.84; std - 0.01\n",
      "37 params - {'clf__tol': 0.0001, 'clf__C': 1000}; mean - 0.84; std - 0.01\n",
      "38 params - {'clf__tol': 0.001, 'clf__C': 1000}; mean - 0.84; std - 0.01\n",
      "39 params - {'clf__tol': 0.01, 'clf__C': 1000}; mean - 0.84; std - 0.01\n",
      "40 params - {'clf__tol': 1, 'clf__C': 1000}; mean - 0.84; std - 0.01\n",
      "41 params - {'clf__tol': 10, 'clf__C': 1000}; mean - 0.63; std - 0.09\n",
      "Done: 2016-11-13 20:05:45\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "parameters =  {'clf__C': [.01,.1,.5,1,10,100,1000],\n",
    "              #tol : float, optional (default=1e-4)  \n",
    "              'clf__tol':[0.00001,0.0001,0.001,0.01,1,10],\n",
    "              # combination hinge,l1 && squared_hinge l1 is not supported\n",
    "              # avoid adjusting loss and penalty and stick with the default values\n",
    "              #'clf__loss': ['hinge','squared_hinge'],\n",
    "              #'clf__penalty':['l1','l2'],\n",
    "              }\n",
    "            \n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=1)\n",
    "grid_search.fit(Xtrain, q3_y_train)\n",
    "\n",
    "# print the mean and std for each candidate along with the parameter\n",
    "# settings for all the candidates explored by grid search.\n",
    "n_candidates = len(grid_search.cv_results_['params'])\n",
    "# print top 10\n",
    "for i in range(n_candidates):\n",
    "    print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
    "             % (grid_search.cv_results_['params'][i],\n",
    "                grid_search.cv_results_['mean_test_score'][i],\n",
    "                gr\n",
    "                id_search.cv_results_['std_test_score'][i]))\n",
    "\n",
    "print(\"Done: \" + lastRunTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from previous run, we see that big tol(>1) produced bad scores\n",
    "very low C:Penalty parameter of the error term(C<=0.1) also  bad scores\n",
    "but no signficant change in the score between low tol or high C. for example we see that these combination gives the same score(the best in our experiments)\n",
    "\n",
    "36 params - {'clf__tol': 1e-05, 'clf__C': 1000}; mean - 0.84; std - 0.01\n",
    "37 params - {'clf__tol': 0.0001, 'clf__C': 1000}; mean - 0.84; std - 0.01\n",
    "38 params - {'clf__tol': 0.001, 'clf__C': 1000}; mean - 0.84; std - 0.01\n",
    "39 params - {'clf__tol': 0.01, 'clf__C': 1000}; mean - 0.84; std - 0.01\n",
    "30 params - {'clf__tol': 1e-05, 'clf__C': 100}; mean - 0.84; std - 0.01\n",
    "31 params - {'clf__tol': 0.0001, 'clf__C': 100}; mean - 0.84; std - 0.01\n",
    "32 params - {'clf__tol': 0.001, 'clf__C': 100}; mean - 0.84; std - 0.01\n",
    "33 params - {'clf__tol': 0.01, 'clf__C': 100}; mean - 0.84; std - 0.01\n",
    "\n",
    "\n",
    "\n",
    "=======> Conclusion based of the best params outcome :\n",
    "no need to change the default tol : 0.0001\n",
    "Change the default C of 1 to 1000. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Classification Report**\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.89      0.90      0.90       245\n",
      "        pos       0.90      0.89      0.90       255\n",
      "\n",
      "avg / total       0.90      0.90      0.90       500\n",
      "\n",
      "**Confusion Matrix**\n",
      "\n",
      "[[221  24]\n",
      " [ 27 228]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAFdCAYAAABGoXXzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2502e4aadd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 2016-11-13 20:05:45\n"
     ]
    }
   ],
   "source": [
    "y_predicted = grid_search.predict(Xtest)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"**Classification Report**\\n\")\n",
    "print(metrics.classification_report(q3_y_test, y_predicted,\n",
    "                                    target_names=dataset.target_names))\n",
    "\n",
    "# Print and plot the confusion matrix\n",
    "print(\"**Confusion Matrix**\\n\")\n",
    "cm = metrics.confusion_matrix(q3_y_test, y_predicted)\n",
    "print(cm)\n",
    "\n",
    "plt.matshow(cm)\n",
    "plt.show()\n",
    "\n",
    "print(\"Done: \" + lastRunTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "** KNeighborsClassifier Examination **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 params - {'clf__p': 1, 'clf__n_neighbors': 1}; mean - 0.50; std - 0.00\n",
      "1 params - {'clf__p': 2, 'clf__n_neighbors': 1}; mean - 0.66; std - 0.01\n",
      "2 params - {'clf__p': 1, 'clf__n_neighbors': 2}; mean - 0.50; std - 0.00\n",
      "3 params - {'clf__p': 2, 'clf__n_neighbors': 2}; mean - 0.65; std - 0.01\n",
      "4 params - {'clf__p': 1, 'clf__n_neighbors': 3}; mean - 0.51; std - 0.01\n",
      "5 params - {'clf__p': 2, 'clf__n_neighbors': 3}; mean - 0.67; std - 0.00\n",
      "6 params - {'clf__p': 1, 'clf__n_neighbors': 4}; mean - 0.52; std - 0.03\n",
      "7 params - {'clf__p': 2, 'clf__n_neighbors': 4}; mean - 0.67; std - 0.01\n",
      "8 params - {'clf__p': 1, 'clf__n_neighbors': 5}; mean - 0.50; std - 0.01\n",
      "9 params - {'clf__p': 2, 'clf__n_neighbors': 5}; mean - 0.65; std - 0.02\n",
      "10 params - {'clf__p': 1, 'clf__n_neighbors': 6}; mean - 0.52; std - 0.02\n",
      "11 params - {'clf__p': 2, 'clf__n_neighbors': 6}; mean - 0.67; std - 0.02\n",
      "12 params - {'clf__p': 1, 'clf__n_neighbors': 7}; mean - 0.52; std - 0.02\n",
      "13 params - {'clf__p': 2, 'clf__n_neighbors': 7}; mean - 0.66; std - 0.01\n",
      "**Classification Report**\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.65      0.68      0.66       248\n",
      "        pos       0.67      0.63      0.65       252\n",
      "\n",
      "avg / total       0.66      0.66      0.66       500\n",
      "\n",
      "**Confusion Matrix**\n",
      "\n",
      "[[168  80]\n",
      " [ 92 160]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAFdCAYAAABGoXXzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ace35f6438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 2016-11-14 09:02:53\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('clf', KNeighborsClassifier())\n",
    "])\n",
    "# find the best n_neighbors and best distance p for minkowski(p=2 -> euclidean, p=1-> manhattan)\n",
    "parameters =  {'clf__n_neighbors': [1,2,3,4,5,6,7],\n",
    "              'clf__p':[1,2],\n",
    "              }\n",
    "            \n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=2)\n",
    "grid_search.fit(Xtrain, q3_y_train)\n",
    "\n",
    "# print the mean and std for each candidate along with the parameter\n",
    "# settings for all the candidates explored by grid search.\n",
    "n_candidates = len(grid_search.cv_results_['params'])\n",
    "# print top 10\n",
    "for i in range(n_candidates):\n",
    "    print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
    "             % (grid_search.cv_results_['params'][i],\n",
    "                grid_search.cv_results_['mean_test_score'][i],\n",
    "                grid_search.cv_results_['std_test_score'][i]))\n",
    "\n",
    "    \n",
    "    \n",
    "y_predicted = grid_search.predict(Xtest)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"**Classification Report**\\n\")\n",
    "print(metrics.classification_report(q3_y_test, y_predicted,\n",
    "                                    target_names=dataset.target_names))\n",
    "\n",
    "# Print and plot the confusion matrix\n",
    "print(\"**Confusion Matrix**\\n\")\n",
    "cm = metrics.confusion_matrix(q3_y_test, y_predicted)\n",
    "print(cm)\n",
    "\n",
    "plt.matshow(cm)\n",
    "plt.show()\n",
    "\n",
    "print(\"Done: \" + lastRunTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__p': 2, 'clf__n_neighbors': 4}\n"
     ]
    }
   ],
   "source": [
    "print (grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best params for KNeighborsClassifier are n_neighbors: 4 and p:2 (euclidean distance).\n",
    "\n",
    "** The result from LinearSVC was way better than KNeighborsClassifier **\n",
    "a good LinearSVC percision imply that there is a good line sperate the reviews,it could be that many instances are close to that hypothetical line from both sides, so when running the KNeighborsClassifier, instances near the line will have close nighbours from instances of the other hypothetical line side. this would cause a high error rate in KNeighborsClassifier that can't fit well to such problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For a particular choice of parameters and classifier, look at 2 examples where the prediction was incorrect.\n",
    "* Can you conjecture on why the classifier made a mistake for this prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:neg 1: pos\n",
      "**Classification Report**\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.89      0.83      0.86       265\n",
      "        pos       0.82      0.89      0.85       235\n",
      "\n",
      "avg / total       0.86      0.86      0.86       500\n",
      "\n",
      "Done: 2016-11-15 14:29:15\n"
     ]
    }
   ],
   "source": [
    "# We will address this problem with the LinearSVC, C=1000 paramater and default for all others.\n",
    "clf  = LinearSVC(C=1000)\n",
    "clf.fit(Xtrain, q3_y_train)\n",
    "y_predicted = clf.predict(Xtest)\n",
    "\n",
    "# lets print the target array so we could know which index refer to positive or negative.\n",
    "print(\"0:\" + dataset.target_names[0] + \" 1: \" + dataset.target_names[1])\n",
    "\n",
    "print(\"**Classification Report**\\n\")\n",
    "print(metrics.classification_report(q3_y_test, y_predicted,\n",
    "                                    target_names=dataset.target_names))\n",
    "\n",
    "print(\"Done: \" + lastRunTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** find false positive **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** false positive example:  1\n",
      "b'i read the new yorker magazine and i enjoy some of their really in-depth articles about some incident . \\nthey will take some incident like the investigation of a mysterious plane crash and tell you what happened in detail . \\nit becomes a real education in what agencies get involved and how theories are suggested , and what kind of pressure the investigators are under , and just about any other aspect you can think of . \\nfrequently i get the feeling that the article sounded exciting , but i am being told in more detail than i really wanted to know . \\noften i get to the middle of a story and say , ok , it sounded good but i now have invested more time than i am willing to spend on this subject . \\nfilm is a different medium . \\nit is a visual medium . \\nthat slows down the telling of stories much more than people realize . \\ni frequently am surprised to find out how short a film script is and how much of the pages are empty space . \\nthe magazine article and the film script are two very different media . \\nthe insider is a film adaptation of the vanity fair article \" the man who knew too much \" by marie brenner . \\nit is too much an adaptation of a magazine article slowed to the pace of a film . \\nit really verges on being tedious at least at times . \\nfor years the seven big companies knew that they dealt in an addictive drug that caused a host of unhealthy side-effects . \\nbut they pretended for the public that it was unproven and they did not really believe it . \\nthe business was incredibly profitable and the proceeds translated into the political power to squelch and discredit any political movements against big tobacco . \\nthe tide turned when a former vice-president of one of the companies was convinced by the cbs 60 minutes news team to tell the public how much the tobacco companies really knew about the health effects of smoking . \\nthe resulting pressure to stop the story created a small civil war at cbs . \\nwho were the major people involved , what were their motives , how was the story almost killed , how did it get aired anyway ? \\nthat is the story covered in surprising detail by the insider . \\nthis all could have been enthralling , but it is not the sort of thing that a stylist like michael mann would be likely to do well . \\nand in the end , he failed . \\nto make a long story short , the film needed a director who knew how to make a long story short . \\nthe film opens with the cbs 60 minutes team in iran with the assignment to interview a terrorist . \\nwe get a taste for their personal style and how they get the upper hand . \\nthey go from being one newsman blindfolded at the hands of the terrorists to the actual interview with mike wallace ( played by christopher plummer ) . \\nthere the news team under producer lowell bergman ( al pacino ) are ordering around the terrorists and getting away with it . \\nthis seems to have nothing to do with the main line of the story , but later when the tobacco industry is so much harder to manipulate than committed terrorists , we have a wry irony on who really has clout in the world . \\nterrorists can grab the headlines , but the tobacco companies have the real position of power . \\nincongruously intercut with the iran interview sequence we see jeffrey wigand ( russell crowe ) dejectedly returning from work to his home . \\nwe discover that he has been fired and his career brought to a complete halt unexpectedly . \\nhe had been a very profitably rewarded vice-president in charge or research and development at brown and williams tobacco ; now he was unemployed and needed money to support his family . \\nrather than support him his wife liane ( diane venora ) demands of him what are they supposed to do for income . \\nmeanwhile the 60 minutes team trying to do a story on fires started by cigarettes have obtained some data they do not understand . \\nthey offer wigand $12 , 000 just to interpret the data . \\nwigand\\'s severance agreement swears him to secrecy about anything he knows about tobacco dealings , but he is reluctantly he stretches the severance terms . \\nhe is willing to read some documents from another tobacco company and interpret them for bergman . \\nin spite of the secrecy , wigand\\'s former employers seem immediately to know wigand is talking to 60 minutes and he is warned off by former boss thomas sandefur played michael gambon in an all too brief but deliciously sinister role . \\nand so the game begins . \\nwigand is irate at his negative treatment for what he still considered continued to be loyalty to his agreement and his former employer . \\nmeanwhile someone is playing very rough with wigand and his family . \\nthe film examines wigand and the pressures placed on his family as they are caught between two powerful giants . \\nwigand has always wanted to make tobacco safer and has natural sympathies with getting the story out . \\nhe and his family are assaulted psychologically and financially by the giant tobacco industry that had never lost a legal fight . \\nal pacino is given top billing but the wigand family is the core of the insider . \\nthe story is told slowly and in just a bit too much meticulous detail . \\nthe film is 157 minutes and is an extremely demanding film for the audience . \\nthe musical score by pieter bourke , lisa gerrard , and graeme revell is one of the worst in recent memory . \\nit puts ominous chords under some scenes and using voice in ways that become a distraction that gets in the way of the storytelling . \\nalso disturbing is the casting of christopher plummer as mike wallace . \\nplummer and wallace are such different types and wallace is too well-known for even so good an actor as plummer to play him convincingly . \\nthis film might have been a really engaging experience under another director\\'s control . \\nmichael mann was the wrong person to helm this film and the insider lacks intensity because of his style . \\ni rate it a 4 on the 0 to 10 scale and a 0 on the -4 to +4 scale . \\n'\n"
     ]
    }
   ],
   "source": [
    "# to find false positive we need to look on 1 in y_predicted that cooresponst to 0 in the q3_y_test\n",
    "for i in range(len(y_predicted)): \n",
    "    if (y_predicted[i] == 1 and q3_y_test[i]==0):\n",
    "        false_positive_idx = i\n",
    "        break     \n",
    "print('** false positive example: ',false_positive_idx)         \n",
    "print(q3_docs_test[false_positive_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "we could conjecture that the model falsely classify this review as positive because it includes some positive structures. even the 1st sentence appears very positive\n",
    "\n",
    "** i read the new yorker magazine and i enjoy some of their really in-depth articles about some incident**\n",
    "** frequently i get the feeling that the article sounded exciting **\n",
    "** for even so good an actor as plummer to play him convincingly**\n",
    "** have been enthralling **\n",
    "\n",
    "We run with (1,2) n grams, so maybe positive terms of two words like \"so good\", \"i enjoy\", \"sounded exciting\" and single words \"exciting\",\"enthralling\",\"good\" that could be more frequent in true positive reviews rather than the negative ones contribute to that the model finds it similar to positive reviews rather than negative.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**find false negative **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** false negative example:  22\n",
      "b'harmless , silly and fun comedy about dim-witted wrestling fans gordie and sean ( david arquette and scott caan ) who idolize current world championship wrestling heavyweight champion jimmy king ( oliver platt ) . \\nwhen king is screwed out of his title by a corrupt promoter ( joe pantoliano ) , gordie and sean take it upon themselves to find their fallen hero and restore his glory . \\nmy biggest fear about ready to rumble was dispatched early on , as the filmmakers are quick to show that wresting is indeed choreographed ( but not fake , mind you ) . \\nthe hook of the movie is that gordie and sean are just too stupid to realize that . \\narquette and caan are suitably over the top with their performances , which is exactly what a movie like this requires , and oliver platt ( one of my favorite actors ) is a riot as the drunken washed-up ex-champion . \\nmany have scoffed at the idea that platt should be playing a heavyweight champion wrestler with an unbeaten record , but for me it just added to the \" silly factor \" of the film , thereby increasing my enjoyment of it . \\none casting complaint however : rose mcgowan as a sexy dancer ? \\nplease . . . \\nif rose mcgowan is sexy then i\\'m marilyn manson . \\ngiven the current state of the actual wcw , if oliver platt were appearing as jimmy king right now on wcw programming , he\\'d be the most popular guy they have . \\non a similar note , the \" plot line \" of the wresting portions of the film are more entertaining than anything the wcw writers have been able to come up with in the last two years . \\nalthough one does have to ask . . . \\nwhy would any wrestling promoter fire the head wrestler of a company who is both unbeaten and extremely popular with the fans ? \\ndirector brian robbins ( you\\'ll remember him as eric from tv\\'s \" head of the class \" ) just knows how to make good dumb movies . \\nthis movie fits in nicely with his previous efforts good burger and varsity blues . \\nand screenwriter steven brill ( the epic mighty ducks trilogy , late last night ) manages to keep things both sophomoric and clever at the same time , with almost all the jokes of the film getting a laugh out of me . \\nthe only exceptions to that were : 1 ) a scene involving a van full of singing nuns and 2 ) any scene involving the old woman wrestling fan . \\nthose moments made me cringe and/or groan . \\nas an added bonus though , the audience is treated to outtakes from the film as the final credits roll . \\n[pg-13] \\n'\n"
     ]
    }
   ],
   "source": [
    "# to find false negative we need to look on 0 in y_predicted that cooresponst to 1 in the q3_y_test\n",
    "for i in range(len(y_predicted)): \n",
    "    if (y_predicted[i] == 0 and q3_y_test[i]==1):\n",
    "        false_negative_idx = i\n",
    "        break\n",
    "print('** false negative example: ', false_negative_idx)     \n",
    "print(q3_docs_test[false_negative_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "we could conjecture that the model falsely classify this review as negative because it includes good amount of 1,2 grams of negative terms like \"silly\", \"corrupt promoter\" , \"screwed out\",\"too stupid\",\"complaint \",\"silly factor\"\n",
    "\n",
    "These terms may be more frequent in true negative reviews rather than the positive ones so they contribute to that the model finds it similar to negative reviews rather than positive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*------------------------\n",
    "\n",
    "## Problem 4 (20 points): Open Ended Question:  Finding the right plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can you find a two dimensional plot in which the positive and negative reviews are separated?\n",
    "    * This problem is hard since you will likely have thousands of features for review, and you will need to transform these thousands of features into just two numbers (so that you can make a 2D plot).\n",
    "* Note, I was not able to find such a plot myself!\n",
    "    * So, this problem is about **trying** but perhaps **not necessarily succeeding**!\n",
    "* I tried two things, neither of which worked very well.\n",
    "    * I first plotted the length of the review versus the number of features we compute that are in that review\n",
    "    * Second I used Principle Component Analysis on a subset of the features.\n",
    "* Can you do better than I did!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report: communicate the results (20 points)\n",
    "\n",
    "(1) (5 points) What data you collected?\n",
    "\n",
    "(2) (5 points) Why this topic is interesting or important to you? (Motivations)\n",
    "\n",
    "(3) (5 points) How did you analyse the data?\n",
    "\n",
    "(4) (5 points) What did you find in the data?\n",
    "(please include figures or tables in the report, but no source code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slides (for 10 minutes of presentation) (20 points)\n",
    "\n",
    "\n",
    "1. (5 points) Motivation about the data collection, why the topic is interesting to you. \n",
    "\n",
    "2. (10 points) Communicating Results (figure/table)\n",
    "\n",
    "3. (5 points) Story telling (How all the parts (data, analysis, result) fit together as a story?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----------------\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "** What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this IPython notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"ipython notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
    "\n",
    "\n",
    "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . We will ask two teams which are randomly selected to present their case studies in class for this case study. \n",
    "\n",
    "* ** Report**: please prepare a report (less than 10 pages) to report what you found in the data.\n",
    "    * What is the relationship between this topic and Business Intelligence?\n",
    "    * How did you analyse the data?\n",
    "    * What did you find in the data? \n",
    "    * What conjectures did you make and how did you support or disprove them using data?\n",
    "    * Did you find anything suprising in the data?\n",
    "    * What business decision do you think this data could help answer?  Why?\n",
    "\n",
    "   (please include figures or tables in the report, **but no source code**)\n",
    "\n",
    "*Please compress all the files into a single zipped file.*\n",
    "\n",
    "\n",
    "** How to submit: **\n",
    "\n",
    "        Send an email to rcpaffenroth@wpi.edu and wliu3@wpi.edu with the subject: \"[DS501] Case study 3-TEAM NUMBER ???\"."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
