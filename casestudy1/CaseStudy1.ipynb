{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 1 : Collecting Data from Twitter\n",
    "\n",
    "Due Date: September 22, **before the beginning of class at 6:00pm**\n",
    "\n",
    "* ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/9/9f/Twitter_bird_logo_2012.svg/220px-Twitter_bird_logo_2012.svg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEAM Members:**\n",
    "\n",
    "    Matthew Beaulieu\n",
    "    \n",
    "    Yousef Fadila\n",
    "    \n",
    "    Meng Li\n",
    "    \n",
    "    Monica Tlachac\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required Readings:** \n",
    "* Chapter 1 and Chapter 9 of the book [Mining the Social Web](http://www.learndatasci.com/wp-content/uploads/2015/08/Mining-the-Social-Web-2nd-Edition.pdf) \n",
    "* The codes for [Chapter 1](http://bit.ly/1qCtMrr) and [Chapter 9](http://bit.ly/1u7eP33)\n",
    "\n",
    "\n",
    "** NOTE **\n",
    "* Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost.\n",
    "\n",
    "*----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Sampling Twitter Data with Streaming API about a certain topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Select a topic that you are interested in, for example, \"WPI\" or \"Lady Gaga\"\n",
    "* Use Twitter Streaming API to sample a collection of tweets about this topic in real time. (It would be recommended that the number of tweets should be larger than 200, but smaller than 1 million.\n",
    "* Store the tweets you downloaded into a local file (txt file or json file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'wikiwords'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6a085ed66a49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mwikiwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtweepy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStream\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstreaming\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStreamListener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'wikiwords'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import warnings\n",
    "import json\n",
    "import numpy as np\n",
    "import wikiwords\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "import pymongo\n",
    "import tweepy\n",
    "import re\n",
    "from prettytable import PrettyTable\n",
    "from collections import Counter\n",
    "\n",
    "warnings.simplefilter(action = \"ignore\", category = FutureWarning)\n",
    "%matplotlib inline\n",
    "#---------------------------------------------\n",
    "# Define a Function to Login Twitter API\n",
    "#\n",
    "# XXX  - DON'T COMMIT TRUE CREDINTIALS TO GITHUB\n",
    "CONSUMER_KEY = 'k01qrrsDSOHdjUObbt1pWjhld'\n",
    "CONSUMER_SECRET ='FTBSa4UU5Nxaw00hNPJQvoAf60HinJM87GfQpuFM0YsVKDdoJD'\n",
    "OAUTH_TOKEN = '4229853605-mSgVKfVSZ9BSN7GbPgvUIPOT1Xl6F3nA2ynKMvy'\n",
    "OAUTH_TOKEN_SECRET = 'jyHtNVKGmqNw4HyBLMYG5i9BT3junKGVFnY6CE2tjJRFW'\n",
    "\n",
    "# mango db auth.\n",
    "MANGODB_SERVER = 'ds033046.mlab.com'\n",
    "MANGODB_PORT = 33046\n",
    "MANGODB_USER = 'ds501u'\n",
    "MANGODB_PASS = 'ds501p'\n",
    "MANGODB_NAME = 'ds501case1'\n",
    "MANGODB_COLLECTION = 'tweets'\n",
    "GEOBOX_USA = [-125,25.1,-60.5,49.1]\n",
    "us_states = [\"WA\" , \"DE\" , \"WI\" , \"WV\" , \"HI\" , \"FL\" , \"WY\" , \"NH\" , \"NJ\" , \"NM\" , \"TX\" , \"LA\" , \"NC\" , \"ND\" , \"NE\" , \"TN\" , \"NY\" , \"PA\" , \"CA\" , \"NV\" , \"VA\" , \"CO\" , \"AK\" , \"AL\" , \"AR\" , \"VT\" , \"IL\" , \"GA\" , \"IN\" , \"IA\" , \"OK\" , \"AZ\" , \"ID\" , \"CT\" , \"ME\" , \"MD\" , \"MA\" , \"OH\" , \"UT\" , \"MO\" , \"MN\" , \"MI\" , \"RI\" , \"KS\" , \"MT\" , \"MS\" , \"SC\" , \"KY\" , \"OR\" , \"SD\" ] \n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
    "\n",
    "db_client = pymongo.MongoClient(MANGODB_SERVER, MANGODB_PORT)\n",
    "db = db_client[MANGODB_NAME]\n",
    "db.authenticate(MANGODB_USER, MANGODB_PASS)\n",
    "tweets_Collection = db[MANGODB_COLLECTION]\n",
    "# Global functions defs----------------------------------------------\n",
    "def print_top_entities(label, entities, max_to_show):\n",
    "    pt = PrettyTable(field_names=[label, 'Count'])\n",
    "    c = Counter(entities)\n",
    "    [ pt.add_row(kv) for kv in c.most_common()[:max_to_show] ]\n",
    "    pt.align[label], pt.align['Count'] = 'l', 'r' # Set column alignment\n",
    "    print(pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyListener(StreamListener):\n",
    "    def __init__(self, max_count = 100):\n",
    "        self.count = 0\n",
    "        self.max_count = max_count   \n",
    "        self.collection = db[MANGODB_COLLECTION]\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            if self.count >= self.max_count:\n",
    "                return False\n",
    "            # the question require output to file. so output to file and save in mangoDB\n",
    "            with open('data/hillary.json', 'a') as f:\n",
    "                f.write(data)\n",
    "            self.count = self.count + 1\n",
    "            datajson = json.loads(data)\n",
    "            self.collection.insert_one(datajson)\n",
    "            return True\n",
    "        except BaseException as e:\n",
    "            print(e)\n",
    "        return True\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    "    \n",
    "api = tweepy.API(auth)\n",
    "limit = 15000\n",
    "twitter_stream = Stream(auth, MyListener(limit))\n",
    "twitter_stream.filter(track=['@HillaryClinton'], languages=[\"en\"], locations=GEOBOX_USA)\n",
    "print(\"streaming done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report some statistics about the tweets you collected \n",
    "\n",
    "* The topic of interest: Hillary Clinton                   \n",
    "\n",
    "\n",
    "* The total number of tweets collected:  ~15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Analyzing Tweets and Tweet Entities with Frequency Analysis\n",
    "\n",
    "**1. Word Count:** \n",
    "* Use the tweets you collected in Problem 1, and compute the frequencies of the words being used in these tweets. \n",
    "* Plot a table of the top 30 words with their counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Text from Tweets** \n",
    "\n",
    "* Get the text column of the tweet df\n",
    "* make it lower case\n",
    "* remove http links and non alphanum characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cursor = tweets_Collection.find()\n",
    "df = pd.DataFrame(list(cursor))\n",
    "\n",
    "text = df.text.str.lower()\n",
    "\n",
    "for i,t in enumerate(text):\n",
    "    t = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", t)\n",
    "    new = ''.join(e for e in t if (e.isalnum() or e==' '))\n",
    "    text[i] = new\n",
    "text = text.str.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dictionary with each unique word as the key, and the number of times it appears as the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "for r in range(text.shape[0]):\n",
    "    lis = text.values[r]\n",
    "    for word in lis:\n",
    "        if word in word_dict.keys():\n",
    "            word_dict[word] +=1\n",
    "        else:\n",
    "            word_dict[word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the key, value pairs to a dictionary (after removing rt, lingering https, and hillaryclinton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = list(stopwords.words('english'))\n",
    "stopwords.extend(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "stopwords.extend(['', ' ', 'rt', 'http','https','hillaryclinton','amp'])\n",
    "vals = []\n",
    "\n",
    "keys = [ e for e in word_dict.keys() if e not in stopwords]\n",
    "for key in keys:\n",
    "    vals.append(word_dict.get(key))\n",
    "    \n",
    "tweet_df = pd.DataFrame({'words':keys, 'vals':vals})\n",
    "tweet_df = tweet_df.sort_values('vals', ascending=False)\n",
    "tweet_df[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon first anlysis, the 30 most used words in the tweets we pulled are pretty common, with special cases such as \"hillary\", and \"trump\".  \n",
    "\n",
    "The next strategy was to normalize each tweet for how popular it is in the English language, to find a more interesting top word count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wikiwords is a cool library that tells how often a word appears in wikipedia articles\n",
    "# and works to normalize for word popularity\n",
    "tweet_df['freq'] = tweet_df['words'].apply(wikiwords.freq)\n",
    "tweet_df = tweet_df[tweet_df['freq'] > 1.0e-6]\n",
    "tweet_df['ratio'] = tweet_df['vals'] / tweet_df['freq']\n",
    "tweet_df.sort_values('ratio', ascending=False)[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list is much more interesting, with words like Benghazi, stunning, hypocrisy, and unfit, seeming like more specific to the subject being discussed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Find the most popular tweets in your collection of tweets**\n",
    "\n",
    "Please plot a table of the top 10 tweets that are the most popular among your collection, i.e., the tweets with the largest number of retweet counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the tweets are already loaded into a Pandas DataFrame, this part should be pretty straightforward..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-362362695dd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'retweet_count'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'False'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'retweet_count'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.sort_values('retweet_count', ascending='False')[['retweet_count', 'text']][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, these tweets were pulled from the twitter streaming API, meaning we saved the tweets as they appeared, and before they were retweeted or favorited by any of the user base.\n",
    "\n",
    "To fix this, we pulled 1/10 of them again from the REST API (not the whole set due to rate limits), saved it to a JSON, loaded it to a df, and ran the command again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/update_hillary.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6dd91ce1b473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/update_hillary.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mlst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/update_hillary.json'"
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "with open('data/update_hillary.json', 'r') as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        lst.append(tweet)\n",
    "new_df = pd.DataFrame(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7d36c4d3ed70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m#drop duplicates is used because on retweeted tweets, twitter stores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;31m# the retweet count from the source tweet, for all of them, so there were several repeats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnew_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'retweet_count'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'retweet_count'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'retweet_count'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'new_df' is not defined"
     ]
    }
   ],
   "source": [
    "#drop duplicates is used because on retweeted tweets, twitter stores\n",
    "# the retweet count from the source tweet, for all of them, so there were several repeats\n",
    "new_df.sort_values('retweet_count', ascending=False)[['user', 'retweet_count', 'text']][0:100].drop_duplicates('retweet_count')[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pulled ~1000 of our tweets from the rest API after they'd existed for a period of time, and could be retweeted. These are the 10 most popular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Find the most popular Tweet Entities in your collection of tweets**\n",
    "\n",
    "Please plot a table of the top 10 hashtags, top 10 user mentions that are the most popular in your collection of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets_Collection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-606e27307ff6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets_Collection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;31m# fetch all screen names from all tweets in the database\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m screen_names = [ user_mention['screen_name'] \n\u001b[1;32m      4\u001b[0m                  \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      for user_mention in tweet['entities']['user_mentions'] ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweets_Collection' is not defined"
     ]
    }
   ],
   "source": [
    "cursor = tweets_Collection.find()\n",
    "# fetch all screen names from all tweets in the database\n",
    "screen_names = [ user_mention['screen_name'] \n",
    "                 for tweet in cursor\n",
    "                     for user_mention in tweet['entities']['user_mentions'] ]\n",
    "\n",
    "print_top_entities('screen name', screen_names, 10)\n",
    "\n",
    "cursor = tweets_Collection.find()\n",
    "#fetch all hashtags form all tweets in the database\n",
    "hashtags = [ hashtag['text'] \n",
    "             for tweet in cursor\n",
    "                 for hashtag in tweet['entities']['hashtags'] ]\n",
    "\n",
    "print_top_entities('hashtag', hashtags, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-25829ab3f169>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# Top hashtags plot\n",
    "\n",
    "c = Counter(hashtags)\n",
    "# plot wordcloud; words size is based on the frequency of each word\n",
    "wordcloud = WordCloud(relative_scaling=0.2, background_color='black',stopwords=stopwords,max_font_size=40\n",
    "                         ).generate_from_frequencies(c.most_common()[:20])\n",
    "\n",
    "plt.figure().set_size_inches(10.5, 15.5)\n",
    "plt.imshow(wordcloud, cmap=plt.cm.gray)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ------------------------\n",
    "\n",
    "# Problem 3: Getting \"All\" friends and \"All\" followers of a popular user in twitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* choose a popular twitter user who has many followers, such as \"ladygaga\".\n",
    "* Get the list of all friends and all followers of the twitter user.\n",
    "* Plot 20 out of the followers, plot their ID numbers and screen names in a table.\n",
    "* Plot 20 out of the friends (if the user has more than 20 friends), plot their ID numbers and screen names in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweepy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-21827f8e77ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m#definitios and config values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mMANGODB_SERVER\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ds035816.mlab.com'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweepy' is not defined"
     ]
    }
   ],
   "source": [
    "#definitios and config values\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "MANGODB_SERVER = 'ds035816.mlab.com'\n",
    "MANGODB_PORT = 35816\n",
    "MANGODB_CLINTON_FF_DB_NAME = \"clintonff\"\n",
    "MANGODB_CLINTON_FF_COLL = \"collection\"\n",
    "db_clinton_ff_client = pymongo.MongoClient(MANGODB_SERVER, MANGODB_PORT)\n",
    "db_clinton_ff = db_clinton_ff_client[MANGODB_CLINTON_FF_DB_NAME]\n",
    "db_clinton_ff.authenticate(MANGODB_USER, MANGODB_PASS)  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweepy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-adceaa330b98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdata_followers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lst'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mfollowers_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_followers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cur' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-adceaa330b98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdb_clinton_ff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mMANGODB_CLINTON_FF_COLL\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfollowers_json\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mdata_followers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lst'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[1;32mexcept\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTweepError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[1;31m# sleep for 15 mintues and then try again after the rate limit is released\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m60\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m15\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweepy' is not defined"
     ]
    }
   ],
   "source": [
    "# fetch all followers \n",
    "import time\n",
    "data_followers = {}\n",
    "data_followers['type'] = 'followers'\n",
    "data_followers['lst'] = []\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        data_followers['lst'] = cur.next()[:]\n",
    "        followers_json = json.loads(json.dumps(data_followers))\n",
    "        db_clinton_ff[MANGODB_CLINTON_FF_COLL].insert_one(followers_json)\n",
    "        del data_followers['lst'][:]     \n",
    "    except tweepy.TweepError:\n",
    "        # sleep for 15 mintues and then try again after the rate limit is released\n",
    "        time.sleep(60 * 15 + 10)\n",
    "        continue\n",
    "    except StopIteration:\n",
    "        break\n",
    "        \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweepy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-36ddad6de1e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfriends_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[1;32mfor\u001b[0m \u001b[0mitm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfriends_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'HillaryClinton'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdata_friends\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lst'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweepy' is not defined"
     ]
    }
   ],
   "source": [
    "# fetch all friends\n",
    "#no need for iteration like fetching followers as number of friends is relatvely small\n",
    "data_friends = {}\n",
    "data_friends['type'] = 'friends'\n",
    "data_friends['lst'] = []\n",
    "\n",
    "# fetch all friends list - as this list relativly samll we won't hit the limit here.\n",
    "\n",
    "friends_ids = []     \n",
    "for itm in tweepy.Cursor(api.friends_ids, id = 'HillaryClinton').items():\n",
    "    data_friends['lst'].append(itm)\n",
    "\n",
    "# save the friend's list in mangoDB    \n",
    "friends_json = json.loads(json.dumps(data_friends))  \n",
    "db_clinton_ff[MANGODB_CLINTON_FF_COLL].insert_one(friends_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Given a set of user ids, this function calls get_screen_names and plots a table of the first (max_ids) ID's and screen names.\n",
    "def draw_pretty_table(ids):\n",
    "    users = api.lookup_users(user_ids=ids)\n",
    "    pretty_table = PrettyTable(['ID','Screen Name'])\n",
    "    [pretty_table.add_row ([row.id,row.screen_name]) for row in users]\n",
    "    pretty_table.align = 'l'\n",
    "    print(pretty_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db_clinton_ff' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9a33283cb0a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# print 20 firends and 20 followers data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb_clinton_ff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mMANGODB_CLINTON_FF_COLL\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"type\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"friends\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n20 friends of Hilary Clinton\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdraw_pretty_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lst'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'db_clinton_ff' is not defined"
     ]
    }
   ],
   "source": [
    "# print 20 firends and 20 followers data\n",
    "#\n",
    "cursor = db_clinton_ff[MANGODB_CLINTON_FF_COLL].find({\"type\": \"friends\"})\n",
    "print (\"\\n20 friends of Hilary Clinton\\n\")\n",
    "draw_pretty_table(cursor[0]['lst'][:20])\n",
    "\n",
    "cursor = db_clinton_ff[MANGODB_CLINTON_FF_COLL].find({\"type\": \"followers\"})\n",
    "print (\"\\n20 followers of Hilary Clinton\\n\")\n",
    "draw_pretty_table(cursor[0]['lst'][:20])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the mutual friends within the two groups, i.e., the users who are in both friend list and follower list, plot their ID numbers and screen names in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db_clinton_ff' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7c94399712be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;31m#   Please feel free to add more cells below this cell if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb_clinton_ff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mMANGODB_CLINTON_FF_COLL\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"type\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"friends\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mfriends\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lst'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb_clinton_ff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mMANGODB_CLINTON_FF_COLL\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"type\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"followers\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'db_clinton_ff' is not defined"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "cursor = db_clinton_ff[MANGODB_CLINTON_FF_COLL].find({\"type\": \"friends\"})\n",
    "friends = cursor[0]['lst']\n",
    "cursor = db_clinton_ff[MANGODB_CLINTON_FF_COLL].find({\"type\": \"followers\"})\n",
    "\n",
    "# we have 1 list of friends (~800 members) and over 500 list of followers; each of 5000 members,\n",
    "#iterate over the followers and build the intersaction list\n",
    "common_friends = []\n",
    "total = 0\n",
    "for follower in cursor:\n",
    "    common_friends.extend(set(friends).intersection(follower['lst']))\n",
    "    total += len(follower['lst'])\n",
    "    \n",
    "#print ('Clinton has {} followers; {} following ; {} mutal friends'.format(total, len(friends), len(common_friends)))\n",
    "print(\"This list is not a complete one as it requires about 30 hours to fetch all followers of \\n Clinto due to twitter rate limit\\n we fetched about 4M out of 8M followers\\n\")\n",
    "print (\"Mutual Friends table\")\n",
    "draw_pretty_table(common_friends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*------------------------\n",
    "\n",
    "# Problem 4: Business question \n",
    "\n",
    "Run some additional experiments with your data to gain familiarity with the twitter data and twitter API.\n",
    "\n",
    "* Come up with a business question that Twitter data could help answer.\n",
    "* Decribe the business case.\n",
    "* How could Twitter data help a company decide how to spend its resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even without complicated analysis twitter can yield impactful results about campaigns.  The 2016 presidential primaries could have been predicted by the amount of tweets about each candidate from the top six news networks (Simms).  With more complicated analyses, information gathered from tweets can not only predict but also guide political campaigns.  As such, campaign managers should take advantage of the information captured by Twitter.\n",
    "\n",
    "Twitter could campaign managers in evaluation the effective of campaign, figure out the concerns of the public in each and different state, find out what top concerns of people against or pro Clinton (analyzing frequency table and top hashtage in positive and negative tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step will be classifiying the tweets to positive and negative, for that we will leverage the http://text-processing.com/ NLTK algorithm and run it on the tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-64fbfcd68aa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msentiment_collection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets_Collection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'db' is not defined"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlencode\n",
    "from urllib.request import Request, urlopen\n",
    "import json\n",
    "\n",
    "sentiment_collection = db['sentiment']\n",
    "\n",
    "cursor = tweets_Collection.find()\n",
    "\n",
    "for document in cursor:\n",
    "    \n",
    "    screen_names = [ user_mention['screen_name']\n",
    "        for user_mention in document['entities']['user_mentions'] ]\n",
    "    # exclude tweets that mention both  realDonaldTrump and clinton from sentiment analysis.\n",
    "    # trying to associate postive/negative in such tweets to clinton or trump is byond this scope of this case.\n",
    "    if 'realDonaldTrump' in screen_names:\n",
    "        continue\n",
    "\n",
    "    # Iterate over the cleaned data - and process sentiment analysis on it. \n",
    "    url = 'https://japerk-text-processing.p.mashape.com/sentiment/' # Set destination URL here\n",
    "    post_fields = {'language': 'english', 'text' : document['text']}     # Set POST fields here\n",
    "\n",
    "    request = Request(url, urlencode(post_fields).encode())\n",
    "    # I put my credit card in this website - first 40000 calls are free - DONT EXCEED \n",
    "    request.add_header('X-Mashape-Key', '1z19gbsrAwmsh1EqYI3CcBbNtjOSp1T1iVIjsnRoFmszvbDkNV')\n",
    "    request.add_header('Content-Type', 'application/x-www-form-urlencoded')\n",
    "    request.add_header('Accept', 'application/json')\n",
    "    data = urlopen(request).read().decode()\n",
    "    datajson = json.loads(data)\n",
    "    sentiment_t = {}\n",
    "    if document['place']:\n",
    "        place = document['place']['full_name'].split()\n",
    "        sentiment_t['state'] = place[(len(place) - 1)]\n",
    "    sentiment_t['text'] = document['text']\n",
    "    sentiment_t['sentiment_label'] = datajson['label']\n",
    "    sentiment_t['entities'] = document['entities']\n",
    "    sentiment_json = json.loads(json.dumps(sentiment_t))\n",
    "    sentiment_collection.insert_one(sentiment_json)\n",
    "\n",
    "print (\"done\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PrettyTable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a1a39995518e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# print sumber of postive/negative postive ratio per state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Postive ratio'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Negative ratio'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPrettyTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maligns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'l'\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PrettyTable' is not defined"
     ]
    }
   ],
   "source": [
    "# print sumber of postive/negative postive ratio per state\n",
    "fields=['state', 'Postive ratio', 'Negative ratio']\n",
    "pt = PrettyTable(fields)\n",
    "pt.aligns = ['l' for f in fields]\n",
    "\n",
    "states_positive_ratio = {}\n",
    "sentiment_collection = db['sentiment']\n",
    "\n",
    "for state in us_states:\n",
    "    row = []\n",
    "    pos_count = sentiment_collection.find({'sentiment_label': 'pos', 'state':state}).count()\n",
    "    neg_count = sentiment_collection.find({'sentiment_label': 'neg', 'state':state}).count()\n",
    "    pos_ratio = 0\n",
    "    neg_ratio = 0\n",
    "    if pos_count + neg_count > 0:\n",
    "        pos_ratio = pos_count / (pos_count + neg_count) * 100\n",
    "        neg_ratio = neg_count / (pos_count + neg_count) * 100\n",
    "    row.append(state)\n",
    "    row.append(\"%.2f\" % pos_ratio)\n",
    "    row.append(\"%.2f\" % neg_ratio)\n",
    "    states_positive_ratio[state] = pos_ratio\n",
    "    pt.add_row(row)\n",
    "print(pt)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'folium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-76b812ba5259>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mfolium\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbase64\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mb64encode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'folium'"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "%matplotlib inline\n",
    "\n",
    "state_geo = r'us-states.json'\n",
    "\n",
    "sentiment_collection = db['sentiment']\n",
    "states_positive_ratio = {}\n",
    "for state in us_states:\n",
    "    pos_count = sentiment_collection.find({'sentiment_label': 'pos', 'state':state}).count()\n",
    "    neg_count = sentiment_collection.find({'sentiment_label': 'neg', 'state':state}).count()\n",
    "    pos_ratio = 0\n",
    "    if pos_count + neg_count > 0:\n",
    "        pos_ratio = pos_count / (pos_count + neg_count) * 100\n",
    "    states_positive_ratio[state] = pos_ratio\n",
    "\n",
    "map_data = states_positive_ratio\n",
    "\n",
    "map = folium.Map(location=(39, -100), zoom_start=4)\n",
    "#map = folium.Map(location=[48, -102], zoom_start=3)\n",
    "map.choropleth(geo_path=state_geo, data=map_data,\n",
    "             key_on='feature.id',\n",
    "             fill_color='BuPu', fill_opacity=0.8, line_opacity=0.6,\n",
    "             reset=False)\n",
    "print (\"the map should be shown below; if you can't see it, please find it in DS501map.html or re-run the first cell and the cell above\")\n",
    "map.save(\"DS501map.html\")\n",
    "HTML(map._repr_html_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO use top entites defined in 2.3\n",
    "def print_top_hashtages(label, tweets, max_to_show):\n",
    "    \n",
    "    hashtags = [ hashtag['text'] \n",
    "         for tweet in tweets\n",
    "             for hashtag in tweet['entities']['hashtags'] ]\n",
    "    pt = PrettyTable(field_names=[label, 'Count'])\n",
    "    c = Counter(hashtags)\n",
    "    [ pt.add_row(kv) for kv in c.most_common()[:max_to_show] ]\n",
    "    pt.align[label], pt.align['Count'] = 'l', 'r' # Set column alignment\n",
    "    print(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b4501b0850e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# print top hastages in postive, negative and netural tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msentiment_collection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentiment_collection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'sentiment_label'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'pos'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint_top_hashtages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Top Hashtage in POSTIVE Tweets'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'db' is not defined"
     ]
    }
   ],
   "source": [
    "# print top hastages in postive, negative and netural tweets\n",
    "\n",
    "sentiment_collection = db['sentiment']\n",
    "cursor = sentiment_collection.find({'sentiment_label': 'pos'})\n",
    "print_top_hashtages('Top Hashtage in POSTIVE Tweets', cursor, 20)\n",
    "\n",
    "cursor = sentiment_collection.find({'sentiment_label': 'neg'})\n",
    "print_top_hashtages('Top Hashtagein NEGATIVE Tweets', cursor, 20)\n",
    "\n",
    "cursor = sentiment_collection.find({'sentiment_label': 'neutral'})\n",
    "print_top_hashtages('Top Hashtage in NEUTRAL Tweets', cursor, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyzing the hashtags distribution between positive and negative We find out that our sentiment NLP algorithm is not well trained - we need to train it again by using part of our data. We could prepare this training set by classifying positive or negative for the training set based on hashtags.\n",
    "For that we decide the make another positive\\negative analysis based on selection set of hashtags from the top ones, this tweets would be used as training set for the NLP algorithm.  due to time constraints, in the scope of this case we will use this subset small test training as our data for the next few steps  without re-training the NLP and running it again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets_Collection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7c5517d112c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;31m# print top frequency words with postive_hashtags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets_Collection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mpositive_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnegative_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweets_Collection' is not defined"
     ]
    }
   ],
   "source": [
    "postive_hashtags = [item.lower() for item in ['ImWithHer','TurnNCBlue','StrongerTogether']]\n",
    "negative_hashtags = [item.lower() for item in['WakeUpAmerica','NeverHillary','MAGA','CrookedHillary','AmericansUnitedForTrump']]\n",
    "\n",
    "# tweet that had both postive_hashtags & negative_hashtags will be ignored.\n",
    "# print top frequency words with postive_hashtags\n",
    "\n",
    "cursor = tweets_Collection.find()\n",
    "positive_tweets = []\n",
    "negative_tweets = []\n",
    "for document in cursor:\n",
    "    hashtags = [ hashtag['text'].lower() \n",
    "         for hashtag in document['entities']['hashtags'] ]\n",
    "    pos_count = len(set(postive_hashtags).intersection(hashtags))\n",
    "    neg_count = len(set(negative_hashtags).intersection(hashtags))\n",
    "    if (pos_count == 0 and neg_count == 0) or (pos_count > 0 and neg_count > 0):\n",
    "        continue\n",
    "    elif pos_count > 0:\n",
    "        positive_tweets.append(document)\n",
    "        #db['pos_version_b'].insert_one(document)\n",
    "    else :\n",
    "        #db['neg_version_b'].insert_one(document)\n",
    "        negative_tweets.append(document)\n",
    "        \n",
    "print(len(positive_tweets))\n",
    "print(len(negative_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These tables for verification only; not that informatic as the classification based on hashtags but it shows the popularity of these tags\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'positive_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-72d8d24198e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"These tables for verification only; not that informatic as the classification based on hashtags but it shows the popularity of these tags\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint_top_hashtages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Top Hashtagein POSITIVE Tweets - VERSION B'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositive_tweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint_top_hashtages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Top Hashtage in NEGATIVE Tweets - VERSION B'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative_tweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'positive_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "# print top hashtags for VERSION B postive/negative tweets\n",
    "\n",
    "print(\"These tables for verification only; not that informatic as the classification based on hashtags but it shows the popularity of these tags\")\n",
    "print_top_hashtages('Top Hashtagein POSITIVE Tweets - VERSION B', positive_tweets, 20)\n",
    "print_top_hashtages('Top Hashtage in NEGATIVE Tweets - VERSION B', negative_tweets, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-fd4a57284c5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# Top negative wordcloud plot\n",
    "\n",
    "hashtags = [ hashtag['text'] \n",
    "     for tweet in negative_tweets\n",
    "         for hashtag in tweet['entities']['hashtags'] ]\n",
    "\n",
    "c = Counter(hashtags)\n",
    "    \n",
    "up_mask = np.array(Image.open(\"down.png\"))\n",
    "\n",
    "stopwords = list(STOPWORDS)\n",
    "stopwords.extend([\"rt\",\"another\",\"wants\",\"noun.\",\"take\",\"good\",\"let's\"])\n",
    "\n",
    "wordcloud = WordCloud(relative_scaling=0.2, mask=up_mask, background_color='black',stopwords=stopwords,max_font_size=40\n",
    "                         ).generate_from_frequencies(c.most_common()[:20])\n",
    "\n",
    "\n",
    "plt.figure().set_size_inches(10.5, 10.5)\n",
    "plt.imshow(wordcloud, cmap=plt.cm.gray)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-5799c6548eba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# Top positive wordcloud plot\n",
    "\n",
    "hashtags = [ hashtag['text'] \n",
    "     for tweet in positive_tweets\n",
    "         for hashtag in tweet['entities']['hashtags'] ]\n",
    "\n",
    "c = Counter(hashtags)\n",
    "    \n",
    "up_mask = np.array(Image.open(\"up.jpg\"))\n",
    "\n",
    "stopwords = list(STOPWORDS)\n",
    "stopwords.extend([\"rt\",\"another\",\"wants\",\"noun.\",\"take\",\"good\",\"let's\"])\n",
    "\n",
    "wordcloud = WordCloud(relative_scaling=0.2, mask=up_mask, background_color='black',stopwords=stopwords,max_font_size=40\n",
    "                         ).generate_from_frequencies(c.most_common()[:20])\n",
    "\n",
    "\n",
    "plt.figure().set_size_inches(10.5, 10.5)\n",
    "plt.imshow(wordcloud, cmap=plt.cm.gray)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'positive_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0be3bd6b20c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m positive_texts = [tweet['text'] \n\u001b[0;32m----> 9\u001b[0;31m                  for tweet in positive_tweets ]\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m positive_words = [ w.lower()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'positive_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "# most common words in positive and negative\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = list(stopwords.words('english'))\n",
    "stopwords.extend(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) \n",
    "\n",
    "\n",
    "positive_texts = [tweet['text'] \n",
    "                 for tweet in positive_tweets ]\n",
    "\n",
    "positive_words = [ w.lower()\n",
    "          for t in positive_texts \n",
    "              for w in t.split() ]\n",
    "\n",
    "positive_words = [w for w in positive_words if w not in stopwords and not w.startswith((\"http\",\"#\",\"@\",\"&\"))]\n",
    "\n",
    "negative_texts = [tweet['text'] \n",
    "                 for tweet in negative_tweets ]\n",
    "negative_words = [ w.lower() \n",
    "          for t in negative_texts \n",
    "              for w in t.split() ]\n",
    "\n",
    "negative_words = [w for w in negative_words if w not in stopwords and not w.startswith((\"http\",\"#\",\"@\",\"&\"))]\n",
    "\n",
    "print_top_entities(\"most frequent words in POSITIVE tweets - VERSION B\", positive_words, 20)\n",
    "print_top_entities(\"most frequent words in NEGATIVE tweets - VERSION B\", negative_words, 20) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# Top negative wordcloud plot\n",
    "c = Counter(negative_words)\n",
    "\n",
    "up_mask = np.array(Image.open(\"down.png\"))\n",
    "\n",
    "stopwords = list(STOPWORDS)\n",
    "stopwords.extend([\"rt\",\"another\",\"wants\",\"noun.\",\"take\",\"good\",\"let's\"])\n",
    "\n",
    "wordcloud = WordCloud(relative_scaling=0.2, mask=up_mask, background_color='black',stopwords=stopwords,max_font_size=40\n",
    "                         ).generate_from_frequencies(c.most_common()[:20])\n",
    "\n",
    "\n",
    "plt.figure().set_size_inches(10.5, 10.5)\n",
    "plt.imshow(wordcloud, cmap=plt.cm.gray)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# Top postive wordcloud plot\n",
    "c = Counter(positive_words)\n",
    "\n",
    "up_mask = np.array(Image.open(\"up.jpg\"))\n",
    "\n",
    "stopwords = list(STOPWORDS)\n",
    "stopwords.extend([\"rt\",\"minutes.\",\"here's\",\"i'm\"])\n",
    "\n",
    "wordcloud = WordCloud(relative_scaling=0.1, mask=up_mask, background_color='black',stopwords=stopwords,max_font_size=40\n",
    "                         ).generate_from_frequencies(c.most_common()[:20])\n",
    "\n",
    "\n",
    "plt.figure().set_size_inches(10.5, 10.5)\n",
    "plt.imshow(wordcloud, cmap=plt.cm.gray)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets calculate postive ratio per state and draw the map again to compare it with the map form the NLP classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lets calculate postive ratio per state and draw the map again to compare it with the map form the NLP classification\n",
    "us_states_positive = {\"WA\"  : 0, \"DE\"  : 0, \"WI\"  : 0, \"WV\"  : 0, \"HI\"  : 0, \"FL\"  : 0, \"WY\"  : 0, \"NH\"  : 0, \"NJ\"  : 0, \"NM\"  : 0, \"TX\"  : 0, \"LA\"  : 0, \"NC\"  : 0, \"ND\"  : 0, \"NE\"  : 0, \"TN\"  : 0, \"NY\"  : 0, \"PA\"  : 0, \"CA\"  : 0, \"NV\"  : 0, \"VA\"  : 0, \"CO\"  : 0, \"AK\"  : 0, \"AL\"  : 0, \"AR\"  : 0, \"VT\"  : 0, \"IL\"  : 0, \"GA\"  : 0, \"IN\"  : 0, \"IA\"  : 0, \"OK\"  : 0, \"AZ\"  : 0, \"ID\"  : 0, \"CT\"  : 0, \"ME\"  : 0, \"MD\"  : 0, \"MA\"  : 0, \"OH\"  : 0, \"UT\"  : 0, \"MO\"  : 0, \"MN\"  : 0, \"MI\"  : 0, \"RI\"  : 0, \"KS\"  : 0, \"MT\"  : 0, \"MS\"  : 0, \"SC\"  : 0, \"KY\"  : 0, \"OR\"  : 0, \"SD\" :0, \"USA\":0 } \n",
    "us_states_nagative = {\"WA\"  : 0, \"DE\"  : 0, \"WI\"  : 0, \"WV\"  : 0, \"HI\"  : 0, \"FL\"  : 0, \"WY\"  : 0, \"NH\"  : 0, \"NJ\"  : 0, \"NM\"  : 0, \"TX\"  : 0, \"LA\"  : 0, \"NC\"  : 0, \"ND\"  : 0, \"NE\"  : 0, \"TN\"  : 0, \"NY\"  : 0, \"PA\"  : 0, \"CA\"  : 0, \"NV\"  : 0, \"VA\"  : 0, \"CO\"  : 0, \"AK\"  : 0, \"AL\"  : 0, \"AR\"  : 0, \"VT\"  : 0, \"IL\"  : 0, \"GA\"  : 0, \"IN\"  : 0, \"IA\"  : 0, \"OK\"  : 0, \"AZ\"  : 0, \"ID\"  : 0, \"CT\"  : 0, \"ME\"  : 0, \"MD\"  : 0, \"MA\"  : 0, \"OH\"  : 0, \"UT\"  : 0, \"MO\"  : 0, \"MN\"  : 0, \"MI\"  : 0, \"RI\"  : 0, \"KS\"  : 0, \"MT\"  : 0, \"MS\"  : 0, \"SC\"  : 0, \"KY\"  : 0, \"OR\"  : 0, \"SD\" :0, \"USA\":0} \n",
    "us_states_ratio =    {\"WA\"  : 0, \"DE\"  : 0, \"WI\"  : 0, \"WV\"  : 0, \"HI\"  : 0, \"FL\"  : 0, \"WY\"  : 0, \"NH\"  : 0, \"NJ\"  : 0, \"NM\"  : 0, \"TX\"  : 0, \"LA\"  : 0, \"NC\"  : 0, \"ND\"  : 0, \"NE\"  : 0, \"TN\"  : 0, \"NY\"  : 0, \"PA\"  : 0, \"CA\"  : 0, \"NV\"  : 0, \"VA\"  : 0, \"CO\"  : 0, \"AK\"  : 0, \"AL\"  : 0, \"AR\"  : 0, \"VT\"  : 0, \"IL\"  : 0, \"GA\"  : 0, \"IN\"  : 0, \"IA\"  : 0, \"OK\"  : 0, \"AZ\"  : 0, \"ID\"  : 0, \"CT\"  : 0, \"ME\"  : 0, \"MD\"  : 0, \"MA\"  : 0, \"OH\"  : 0, \"UT\"  : 0, \"MO\"  : 0, \"MN\"  : 0, \"MI\"  : 0, \"RI\"  : 0, \"KS\"  : 0, \"MT\"  : 0, \"MS\"  : 0, \"SC\"  : 0, \"KY\"  : 0, \"OR\"  : 0, \"SD\" :0, \"USA\":0} \n",
    "\n",
    "crusor = db['pos_version_b'].find()\n",
    "for document in crusor:\n",
    "    if document['place']:\n",
    "        place = document['place']['full_name'].split()\n",
    "        state = place[(len(place) - 1)]\n",
    "        us_states_positive[state] = us_states_positive[state] + 1 \n",
    "\n",
    "crusor = db['neg_version_b'].find()\n",
    "for document in crusor:\n",
    "    if document['place']:\n",
    "        place = document['place']['full_name'].split()\n",
    "        state = place[(len(place) - 1)]\n",
    "        us_states_nagative[state] = us_states_positive[state] + 1 \n",
    "\n",
    "for state in us_states:\n",
    "    if us_states_positive[state] + us_states_nagative[state] != 0:\n",
    "        us_states_ratio[state] = 100 * us_states_positive[state] / (us_states_positive[state] + us_states_nagative[state] )\n",
    "\n",
    "map_data = us_states_ratio\n",
    "\n",
    "map = folium.Map(location=(39, -100), zoom_start=4)\n",
    "#map = folium.Map(location=[48, -102], zoom_start=3)\n",
    "map.choropleth(geo_path=state_geo, data=map_data,\n",
    "             key_on='feature.id',\n",
    "             fill_color='BuPu', fill_opacity=0.8, line_opacity=0.6,\n",
    "             reset=False)\n",
    "print (\"the map should be shown below; if you can't see it, please find it in DS501map_ver_b.html or re-run the first cell and the cell above\")\n",
    "map.save(\"DS501map_ver_b.html\")\n",
    "print(\"As number of tweets we classify by tag and has geo info is very small (less than 20) many states are with ratio 0; there is a need to collect more data\")\n",
    "HTML(map._repr_html_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----------------\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "** What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this IPython notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"ipython notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
    "\n",
    "\n",
    "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . We will ask two teams which are randomly selected to present their case studies in class for this case study. \n",
    "\n",
    "* ** Report**: please prepare a report (less than 10 pages) to report what you found in the data.\n",
    "    * What data you collected? \n",
    "    * Why this topic is interesting or important to you? (Motivations)\n",
    "    * How did you analyse the data?\n",
    "    * What did you find in the data? \n",
    " \n",
    "     (please include figures or tables in the report, but no source code)\n",
    "\n",
    "Please compress all the files in a zipped file.\n",
    "\n",
    "\n",
    "** How to submit: **\n",
    "\n",
    "        Please submit through email to Prof. Paffenroth (rcpaffenroth@wpi.edu) *and* the TA Wen Liu (wliu3@wpi.edu).\n",
    "        \n",
    "** Note: Each team just needs to submits one submission **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading Criteria:\n",
    "\n",
    "** Totoal Points: 120 **\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "** Notebook:  **\n",
    "    Points: 80\n",
    "\n",
    "\n",
    "    -----------------------------------\n",
    "    Qestion 1:\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "    \n",
    "    (1) Select a topic that you are interested in.\n",
    "    Points: 6 \n",
    "    \n",
    "    (2) Use Twitter Streaming API to sample a collection of tweets about this topic in real time. (It would be recommended that the number of tweets should be larger than 200, but smaller than 1 million. Please check whether the total number of tweets collected is larger than 200?\n",
    "    Points: 10 \n",
    "    \n",
    "    \n",
    "    (3) Store the tweets you downloaded into a local file (txt file or json file)\n",
    "    Points: 4 \n",
    "    \n",
    "    \n",
    "    -----------------------------------\n",
    "    Qestion 2:\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "    \n",
    "    1. Word Count\n",
    "\n",
    "    (1) Use the tweets you collected in Problem 1, and compute the frequencies of the words being used in these tweets.\n",
    "    Points: 4 \n",
    "\n",
    "    (2) Plot a table of the top 30 words with their counts \n",
    "    Points: 4 \n",
    "    \n",
    "    2. Find the most popular tweets in your collection of tweets\n",
    "    plot a table of the top 10 tweets that are the most popular among your collection, i.e., the tweets with the largest number of retweet counts.\n",
    "    Points: 4 \n",
    "    \n",
    "    3. Find the most popular Tweet Entities in your collection of tweets\n",
    "\n",
    "    (1) plot a table of the top 10 hashtags, \n",
    "    Points: 4 \n",
    "\n",
    "    (2) top 10 user mentions that are the most popular in your collection of tweets.\n",
    "    Points: 4 \n",
    "    \n",
    "    \n",
    "    -----------------------------------\n",
    "    Qestion 3:\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "    \n",
    "    (1) choose a popular twitter user who has many followers, such as \"ladygaga\".\n",
    "    Points: 4 \n",
    "\n",
    "    (2) Get the list of all friends and all followers of the twitter user.\n",
    "    Points: 4 \n",
    "\n",
    "    (3) Plot 20 out of the followers, plot their ID numbers and screen names in a table.\n",
    "    Points: 4 \n",
    "\n",
    "    (4) Plot 20 out of the friends (if the user has more than 20 friends), plot their ID numbers and screen names in a table.\n",
    "    Points: 4 \n",
    "    \n",
    "    (5) Compute the mutual friends within the two groups, i.e., the users who are in both friend list and follower list, plot their ID numbers and screen names in a table\n",
    "    Points: 4 \n",
    "  \n",
    "    -----------------------------------\n",
    "    Qestion 4:  Business question\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "        Novelty: 10\n",
    "        Interestingness: 10\n",
    "    -----------------------------------\n",
    "    Run some additional experiments with your data to gain familiarity with the twitter data ant twitter API.  Come up with a business question and describe how Twitter data can help you answer that question.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "** Report: communicate the results**\n",
    "    Points: 20\n",
    "\n",
    "(1) What data you collected?\n",
    "    Points: 5 \n",
    "\n",
    "(2) Why this topic is interesting or important to you? (Motivations)\n",
    "    Points: 5 \n",
    "\n",
    "(3) How did you analyse the data?\n",
    "    Points: 5 \n",
    "\n",
    "(4) What did you find in the data?\n",
    "(please include figures or tables in the report, but no source code)\n",
    "    Points: 5 \n",
    "\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "** Slides (for 10 minutes of presentation): Story-telling **\n",
    "    Points: 20\n",
    "\n",
    "\n",
    "1. Motivation about the data collection, why the topic is interesting to you.\n",
    "    Points: 5 \n",
    "\n",
    "2. Communicating Results (figure/table)\n",
    "    Points: 10 \n",
    "\n",
    "3. Story telling (How all the parts (data, analysis, result) fit together as a story?)\n",
    "    Points: 5 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
